{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK 1 DEMOSTRATING COCA CORPUS\n",
    "'''The Corpus of Contemporary American English (COCA) is the only large, genre-balanced corpus of American English.\n",
    "COCA is probably the most widely-used corpus of English, \n",
    "and it is related to many other corpora of English that we have created, \n",
    "which offer unparalleled insight into variation in English.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lemmat'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TASK 2 STEMMING (PorterStemmer):\n",
    "\n",
    "'''Stemming is basically removing the suffix from a word and reduce it to its root word.\n",
    "For example: “Flying” is a word and its suffix is “ing”,\n",
    "if we remove “ing” from “Flying” then we will get base word or root word which is “Fly”.\n",
    "We uses these suffix to create a new word from original stem word.'''\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "Stemmerporter=PorterStemmer()\n",
    "Stemmerporter.stem('lemmatization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cheer'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TASK 2 STEMMING:\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "Stemmerporter=PorterStemmer()\n",
    "Stemmerporter.stem('cheerfulness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'technolog'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LancasterStemmer\n",
    "\n",
    "'''Lancaster: Very aggressive stemming algorithm, sometimes to a fault.\n",
    "With porter and snowball, the stemmed representations are usually fairly intuitive to a reader, \n",
    "not so with Lancaster, as many shorter words will become totally obfuscated.'''\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmerLan =LancasterStemmer()\n",
    "stemmerLan.stem('technology')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ing'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RegexpStemmer\n",
    "\n",
    "'''A stemmer that uses regular expressions to identify morphological affixes. \n",
    "Any substrings that match the regular expressions will be removed.'''\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmerregexp=RegexpStemmer('learn')\n",
    "stemmerregexp.stem('learning')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mang'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SnowballStemmer : Snowball is a small string processing language designed for creating stemming algorithms for use in Information Retrieval.\n",
    "\n",
    "# Include multiple languages.\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages\n",
    "frenchstemmer=SnowballStemmer('french')\n",
    "frenchstemmer.stem('manges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Am quick brown fox jump over a lazi dog\n"
     ]
    }
   ],
   "source": [
    "#TASK 3: STEMMING PARAGRAPHS\n",
    "\n",
    "# Here all the individual tokens of the sentence are reduced to their stems.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "example = \"Am quick brown fox jumps over a lazy dog\"\n",
    "example = [stemmer.stem(token) for token in example.split(\" \")]\n",
    "print (\" \".join(example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cactus\n",
      "mouse\n",
      "rock\n",
      "good\n",
      "Am\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "# TASK 4: LEMMATIZER\n",
    "'''Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
    "Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.'''\n",
    "\n",
    "\n",
    "'''Text preprocessing includes both Stemming as well as Lemmatization. \n",
    "Many times people find these two terms confusing. Some treat these two as same.\n",
    "Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.'''\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"mice\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"better\", pos = 'a')) # given the part-of-speech, better lemmatizes to good\n",
    "\n",
    "print(lemmatizer.lemmatize(\"Am\"))   # This error is fixed when the PArt of Speech is given\n",
    "\n",
    "print(lemmatizer.lemmatize(\"am\", pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\91814\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.431 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "把 句子 中所 所有 的 可以 成 词 的 词语 都 扫描 描出 描出来 出来\n"
     ]
    }
   ],
   "source": [
    "# TASK 5: CHINESE SEGMENTATION USING JIEBA\n",
    "\n",
    "import jieba\n",
    "seg = jieba.cut(\"把句子中所有的可以成词的词语都扫描出来\", cut_all = True)\n",
    "print(\" \".join(seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Become', 'an', 'expert', 'in', 'NLP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91814\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\91814\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# TASK 6: BASIC TEXT PROCESSING PIPELINE\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import nltk\n",
    "sent = \"Become an expert in NLP\"\n",
    "words = nltk.word_tokenize(sent)  #Tokenizer just seprate apart the words or Tokens of a sentence.\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'only', 'true', 'wisdom', 'is', 'in', 'knowin', \"'\", 'you', 'know', 'nothing', '.']\n",
      "[('The', 'DT'), ('only', 'JJ'), ('true', 'JJ'), ('wisdom', 'NN'), ('is', 'VBZ'), ('in', 'IN'), ('knowin', 'NN'), (\"'\", \"''\"), ('you', 'PRP'), ('know', 'VBP'), ('nothing', 'NN'), ('.', '.')]\n",
      "['Beware', 'the', 'barrenness', 'of', 'a', 'busy', 'life', '.']\n",
      "[('Beware', 'NNP'), ('the', 'DT'), ('barrenness', 'NN'), ('of', 'IN'), ('a', 'DT'), ('busy', 'JJ'), ('life', 'NN'), ('.', '.')]\n",
      "['I', 'decided', 'that', 'it', 'was', 'not', 'wisdom', 'that', 'enabled', 'poets', 'to', 'write', 'their', 'poetry', ',', 'but', 'a', 'kind', 'of', 'ins', '.']\n",
      "[('I', 'PRP'), ('decided', 'VBD'), ('that', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('not', 'RB'), ('wisdom', 'JJ'), ('that', 'IN'), ('enabled', 'VBD'), ('poets', 'NNS'), ('to', 'TO'), ('write', 'VB'), ('their', 'PRP$'), ('poetry', 'NN'), (',', ','), ('but', 'CC'), ('a', 'DT'), ('kind', 'NN'), ('of', 'IN'), ('ins', 'NNS'), ('.', '.')]\n",
      "['or', 'inspiration', ',', 'such', 'as', 'you', 'find', 'in', 'seers', 'and', 'prophets', 'who', 'deliver', 'all', 'their', 'sublime', 'messages', 'without', 'knowing', 'in', 'the', 'least', 'what', 'they', 'mean', '.']\n",
      "[('or', 'CC'), ('inspiration', 'NN'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('you', 'PRP'), ('find', 'VBP'), ('in', 'IN'), ('seers', 'NNS'), ('and', 'CC'), ('prophets', 'NNS'), ('who', 'WP'), ('deliver', 'VBP'), ('all', 'DT'), ('their', 'PRP$'), ('sublime', 'NN'), ('messages', 'NNS'), ('without', 'IN'), ('knowing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('least', 'JJS'), ('what', 'WP'), ('they', 'PRP'), ('mean', 'VBP'), ('.', '.')]\n",
      "['Be', 'as', 'you', 'wish', 'to', 'seem', '.']\n",
      "[('Be', 'VB'), ('as', 'IN'), ('you', 'PRP'), ('wish', 'VBP'), ('to', 'TO'), ('seem', 'VB'), ('.', '.')]\n",
      "['Wonder', 'is', 'the', 'beginning', 'of', 'wisdom', '.']\n",
      "[('Wonder', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('beginning', 'NN'), ('of', 'IN'), ('wisdom', 'NN'), ('.', '.')]\n",
      "['Be', 'kind', ',', 'for', 'everyone', 'you', 'meet', 'is', 'fighting', 'a', 'hard', 'battle', '.']\n",
      "[('Be', 'NNP'), ('kind', 'NN'), (',', ','), ('for', 'IN'), ('everyone', 'NN'), ('you', 'PRP'), ('meet', 'VBP'), ('is', 'VBZ'), ('fighting', 'VBG'), ('a', 'DT'), ('hard', 'JJ'), ('battle', 'NN'), ('.', '.')]\n",
      "['Our', 'prayers', 'should', 'be', 'for', 'blessings', 'in', 'general', ',', 'for', 'God', 'knows', 'best', 'what', 'is', 'good', 'for', 'us', '.']\n",
      "[('Our', 'PRP$'), ('prayers', 'NNS'), ('should', 'MD'), ('be', 'VB'), ('for', 'IN'), ('blessings', 'NNS'), ('in', 'IN'), ('general', 'JJ'), (',', ','), ('for', 'IN'), ('God', 'NNP'), ('knows', 'VBZ'), ('best', 'JJS'), ('what', 'WP'), ('is', 'VBZ'), ('good', 'JJ'), ('for', 'IN'), ('us', 'PRP'), ('.', '.')]\n",
      "total word [100]\n"
     ]
    }
   ],
   "source": [
    "texts = [\"\"\"The only true wisdom is in knowin' you know nothing.\n",
    "Beware the barrenness of a busy life.\n",
    "I decided that it was not wisdom that enabled poets to write their poetry,\n",
    "but a kind of ins. or inspiration, such as you find in seers and prophets who deliver all their sublime messages without knowing in the least what they mean. Be as you wish to seem. Wonder is the beginning of wisdom. Be kind, for everyone you meet is fighting a hard battle.  Our prayers should be for blessings in general, for God knows best what is good for us.\"\"\"]\n",
    "count=0;\n",
    "for text in texts:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        print(words)\n",
    "        tagged = nltk.pos_tag(words) #This taggs the part of speech as we can see that in output.\n",
    "        print(tagged)\n",
    "num_words = [len(sentence.split()) for sentence in texts]\n",
    "print('total word',num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'s': 9, 'o': 8, 'r': 7, 'e': 6, 'n': 4, 'u': 3, 'a': 3, 'l': 3, 'd': 3, 'b': 3, 'f': 3, 'i': 3, 'g': 3, 'h': 2, 'w': 2, 't': 2, 'O': 1, 'p': 1, 'y': 1, ',': 1, 'G': 1, 'k': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "for word in words:\n",
    "    my_counter.update(word)\n",
    "print(my_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
